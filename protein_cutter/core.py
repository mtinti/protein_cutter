"""Core utilities for in-silico protein digestion and peptide analysis"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_core.ipynb.

# %% auto 0
__all__ = ['TEST_DATA', 'protein_dict', 'load_fasta', 'digest', 'digest_to_set', 'fasta_to_peptide_set',
           'annotate_peptides_inplace']

# %% ../nbs/00_core.ipynb 3
from typing import Dict, Union
from pyteomics import parser
from pyteomics import mass
from pathlib import Path
from Bio import SeqIO
import pandas as pd

# %% ../nbs/00_core.ipynb 4
from pathlib import Path
import os

# Get the repository root
if 'GITHUB_WORKSPACE' in os.environ:
    # In GitHub Actions
    REPO_ROOT = Path(os.environ['GITHUB_WORKSPACE'])
else:
    # Local development - find repo root
    REPO_ROOT = Path.cwd()
    while not (REPO_ROOT / 'settings.ini').exists():
        if REPO_ROOT == REPO_ROOT.parent:
            REPO_ROOT = Path.cwd()  # Fallback
            break
        REPO_ROOT = REPO_ROOT.parent

TEST_DATA = REPO_ROOT / 'test_data'

print(f"Repo root: {REPO_ROOT}")
print(f"Test data dir: {TEST_DATA}")
print(f"Test data exists: {TEST_DATA.exists()}")

# %% ../nbs/00_core.ipynb 5
def load_fasta(fasta_path: Union[str, Path]) -> Dict[str, str]:
    """
    Load a FASTA file and return a dictionary mapping protein IDs to sequences.
    
    Parameters
    ----------
    fasta_path : str or Path
        Path to the FASTA file to load.
    
    Returns
    -------
    Dict[str, str]
        Dictionary mapping protein IDs (record.id) to amino acid sequences (as strings).
    
    Raises
    ------
    FileNotFoundError
        If the specified FASTA file does not exist.
    ValueError
        If the file is empty or cannot be parsed as FASTA format.
    
    Examples
    --------
    >>> proteins = load_fasta("proteins.fasta")
    >>> len(proteins)
    42
    >>> proteins['sp|P12345|EXAMPLE']
    'MKTAYIAKQRQISFVKSHFSRQLEERLGL...'
    """
    import os
    path = os.path.abspath(os.curdir)
    print(path)
    
    fasta_path = Path(fasta_path)
    if not fasta_path.exists():
        raise FileNotFoundError(f"FASTA file not found: {fasta_path} {path}")
    
    protein_dict = {}
    with fasta_path.open('r') as handle:
        for record in SeqIO.parse(handle, "fasta"):
            protein_dict[record.id] = str(record.seq)
    
    if not protein_dict:
        raise ValueError(f"No sequences found in FASTA file: {fasta_path} {path}")
    
    return protein_dict

# %% ../nbs/00_core.ipynb 6
protein_dict = load_fasta(TEST_DATA / 'test_sequence.fa')

# %% ../nbs/00_core.ipynb 8
def digest(
    sequence: str,
    protein_id: str,
    enzyme: str = 'trypsin',
    missed_cleavages: int = 1,
    charge_states: list = [1, 2, 3],
    mass_range: tuple = (800.0, 4000.0),
    min_pep_length: int = 5,
    max_pep_length: int = 35,
    sort_by_mass: bool = False,
) -> pd.DataFrame:
    """
    Digest a protein and add flanking amino acids for each peptide.
    
    Parameters
    ----------
    sequence : str
        Protein sequence to digest
    protein_id : str
        Protein identifier (for the DataFrame)
    enzyme : str
        Enzyme name (default: 'trypsin')
    missed_cleavages : int
        Number of allowed missed cleavages (default: 1)
    charge_states : list
        Charge states for m/z calculation (default: [1, 2, 3])
    mass_range : tuple
        (min, max) monoisotopic mass filter in Da (default: (800.0, 4000.0))
    min_pep_length : int
        Minimum peptide length to retain (default: 5)
    max_pep_length : int
        Maximum peptide length to retain (default: 35)
    sort_by_mass : bool
        Sort output by monoisotopic mass (default: False)
    
    Returns
    -------
    pd.DataFrame
        DataFrame with the following columns:
        - start_index : int - Start position in protein sequence
        - end_index : int - End position in protein sequence
        - pep_seq : str - Peptide sequence
        - protein_id : str - Protein identifier
        - pep_length : int - Peptide length
        - prev_aa : str - Previous amino acid (or '-' at N-terminus)
        - next_aa : str - Next amino acid (or '-' at C-terminus)
        - extended_seq : str - Sequence with flanking AAs
        - rep_extended_seq : str - Extended sequence with parentheses
        - mass_mono : float - Monoisotopic mass (Da)
        - mass_avg : float - Average mass (Da)
        - mz_{z} : float - m/z for each charge state in charge_states
    """
    # Digest the protein
    cleavage_results = parser.xcleave(
        sequence,
        enzyme,
        missed_cleavages=missed_cleavages
    )
    
    # Create DataFrame with proper dtypes
    df = pd.DataFrame(
        cleavage_results,
        columns=['start_index', 'pep_seq']
    )
    
    # Add protein ID
    df['protein_id'] = protein_id
    
    # Calculate end index
    df['end_index'] = df['start_index'] + df['pep_seq'].str.len()
    
    # Add peptide length
    df['pep_length'] = df['pep_seq'].str.len()
    
    # Get flanking amino acids with proper boundary handling
    df['prev_aa'] = df['start_index'].apply(
        lambda idx: sequence[idx - 1] if idx > 0 else '-'
    )
    
    df['next_aa'] = df['end_index'].apply(
        lambda idx: sequence[idx] if idx < len(sequence) else '-'
    )
    
    # Create extended sequence (prev-peptide-next)
    df['extended_seq'] = df['prev_aa'] + df['pep_seq'] + df['next_aa']
    df['rep_extended_seq'] = '(' + df['prev_aa'] + ')' + df['pep_seq'] + '(' + df['next_aa'] + ')'
    
    # Calculate masses
    df['mass_mono'] = df['pep_seq'].apply(mass.fast_mass)
    df['mass_avg'] = df['pep_seq'].apply(
        lambda seq: mass.calculate_mass(seq, average=True)
    )
    
    # Calculate m/z for different charge states
    for z in charge_states:
        df[f'mz_{z}'] = df['pep_seq'].apply(
            lambda seq: mass.calculate_mass(seq, charge=z)
        )
    
    # Build column list dynamically based on charge_states
    cols = [
        'start_index', 'end_index', 'pep_seq', 'protein_id', 'pep_length',
        'prev_aa', 'next_aa', 'extended_seq', 'rep_extended_seq',
        'mass_mono', 'mass_avg'
    ]
    cols += [f'mz_{z}' for z in charge_states]
    df = df[cols]
    
    # Apply filters
    df = df[
        (df['pep_seq'].str.len() >= min_pep_length) &
        (df['pep_seq'].str.len() <= max_pep_length) &
        (df['mass_mono'].between(mass_range[0], mass_range[1]))
    ]
    
    if sort_by_mass:
        df = df.sort_values('mass_mono')
    
    return df

# %% ../nbs/00_core.ipynb 19
def digest_to_set(
    sequence: str,
    enzyme: str = 'trypsin',
    missed_cleavages: int = 0,
    mass_range: tuple[float, float] = (800.0, 4000.0),
    min_pep_length: int = 5,
    max_pep_length: int = 35,
) -> set[str]:
    """
    Lightweight digest that returns only peptide sequences as a set.
    
    This is a memory-efficient alternative to `digest()` when only the
    peptide sequences are needed, without positional or mass annotations.
    
    Parameters
    ----------
    sequence : str
        Protein sequence to digest
    enzyme : str
        Enzyme name (default: 'trypsin')
    missed_cleavages : int
        Number of allowed missed cleavages (default: 0)
    mass_range : tuple[float, float]
        (min, max) monoisotopic mass filter in Da (default: (800.0, 4000.0))
    min_pep_length : int
        Minimum peptide length to retain (default: 5)
    max_pep_length : int
        Maximum peptide length to retain (default: 35)  
        
    Returns
    -------
    set[str]
        Set of peptide sequences passing the length and mass filters
    
    Examples
    --------
    >>> peptides = digest_to_set("MKTAYIAKQRQISFVKSHFSRQLEERLGLIEVQAPILSRVGDGTQDNLSGAEKAVQVKVK")
    >>> len(peptides)
    2
    >>> "LGLIEVQAPILSR" in peptides
    True
    """
    # Digest the protein
    cleavage_results = parser.xcleave(
        sequence,
        enzyme,
        missed_cleavages=missed_cleavages
    )
    
    # Filter and return as set
    return {
        pep_seq
        for _, pep_seq in cleavage_results
        if len(pep_seq) >= min_pep_length
        and len(pep_seq) <= max_pep_length
        and mass_range[0] <= mass.fast_mass(pep_seq) <= mass_range[1]
    }

# %% ../nbs/00_core.ipynb 22
from typing import Union

def fasta_to_peptide_set(
    fasta_path: Union[str, Path],
    enzyme: str = 'trypsin',
    missed_cleavages: int = 0,
    mass_range: tuple[float, float] = (800.0, 4000.0),
    min_pep_length: int = 5,
    max_pep_length: int = 35,
    show_progress: bool = True,
) -> set[str]:
    """
    Parse a FASTA file and return all unique peptides as a set.
    
    Memory-efficient function that digests proteins on-the-fly without
    storing the full protein sequences. Useful for building canonical
    peptide reference sets for comparison with experimental data.
    
    Parameters
    ----------
    fasta_path : str or Path
        Path to the FASTA file to parse
    enzyme : str
        Enzyme name (default: 'trypsin')
    missed_cleavages : int
        Number of allowed missed cleavages (default: 0)
    mass_range : tuple[float, float]
        (min, max) monoisotopic mass filter in Da (default: (800.0, 4000.0))
    min_pep_length : int
        Minimum peptide length to retain (default: 5)
    max_pep_length : int
        Maximum peptide length to retain (default: 35)        
    show_progress : bool
        Show progress bar with tqdm (default: True)
    
    Returns
    -------
    set[str]
        Set of all unique peptide sequences from the FASTA file
    
    Raises
    ------
    FileNotFoundError
        If the specified FASTA file does not exist.
    """
    fasta_path = Path(fasta_path)
    if not fasta_path.exists():
        raise FileNotFoundError(f"FASTA file not found: {fasta_path}")
    
    peptide_set = set()
    
    with fasta_path.open('r') as handle:
        records = SeqIO.parse(handle, "fasta")
        if show_progress:
            from tqdm import tqdm
            records = tqdm(records, desc="Digesting proteins")
        
        for record in records:
            peptide_set.update(
                digest_to_set(
                    sequence=str(record.seq),
                    enzyme=enzyme,
                    missed_cleavages=missed_cleavages,
                    mass_range=mass_range,
                    min_pep_length=min_pep_length,
                    max_pep_length=max_pep_length
                )
            )
    
    return peptide_set

# %% ../nbs/00_core.ipynb 26
import shutil
import tempfile

def annotate_peptides_inplace(
    file_path: Union[str, Path],
    canonical_peptides: set[str],
    sequence_col: str = 'PEP.StrippedSequence',
    new_col_name: str = 'is_canonical',
    sep: str = None,
    show_progress: bool = True,
) -> int:
    """
    Annotate peptides in-place by writing to temp file then replacing.
    
    Parameters
    ----------
    file_path : str or Path
        Path to peptide file (will be modified in-place)
    canonical_peptides : set[str]
        Set of canonical peptide sequences
    sequence_col : str
        Column name containing peptide sequences (default: 'PEP.StrippedSequence')
    new_col_name : str
        Name for the new annotation column (default: 'is_canonical')
    sep : str or None
        Field separator. If None, auto-detects from extension (default: None)
    show_progress : bool
        Show progress bar (default: True)
    
    Returns
    -------
    int
        Number of peptides processed
    
    Raises
    ------
    FileNotFoundError
        If the input file does not exist.
    ValueError
        If sequence_col is not found or new_col_name already exists.
    """
    file_path = Path(file_path)
    
    if not file_path.exists():
        raise FileNotFoundError(f"File not found: {file_path}")
    
    # Auto-detect separator
    if sep is None:
        sep = ',' if file_path.suffix == '.csv' else '\t'
    
    # Check header before processing
    with file_path.open('r') as f:
        header = f.readline().rstrip('\n\r')
        columns = header.split(sep)
        
        if new_col_name in columns:
            raise ValueError(f"Column '{new_col_name}' already exists in file")
        
        if sequence_col not in columns:
            raise ValueError(f"Column '{sequence_col}' not found. Available: {columns[:10]}...")
    
    # Create temp file in same directory
    temp_fd, temp_path = tempfile.mkstemp(
        dir=file_path.parent,
        suffix='.tmp'
    )
    temp_path = Path(temp_path)
    
    try:
        peptide_count = 0
        seq_col_idx = columns.index(sequence_col)
        
        with file_path.open('r') as infile, open(temp_fd, 'w') as outfile:
            # Skip header (already read) and write new header
            infile.readline()
            outfile.write(f"{header}{sep}{new_col_name}\n")
            
            # Setup progress bar
            lines = infile
            if show_progress:
                from tqdm import tqdm
                lines = tqdm(infile, desc="Annotating peptides")
            
            for line in lines:
                line = line.rstrip('\n\r')
                fields = line.split(sep)
                
                peptide = fields[seq_col_idx]
                is_canonical = peptide in canonical_peptides
                
                outfile.write(f"{line}{sep}{is_canonical}\n")
                peptide_count += 1
        
        # Atomic replace
        shutil.move(temp_path, file_path)
        
    except Exception:
        if temp_path.exists():
            temp_path.unlink()
        raise
    
    return peptide_count
